[build-system]
requires = ["setuptools>=65.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "aurora"
version = "1.0.0"
description = "Intelligent Voice Assistant for Local Automation and Productivity"
readme = "readme.md"
license = {file = "LICENSE"}
authors = [
    {name = "Aurora Team"}
]
keywords = [
    "voice-assistant", 
    "ai", 
    "automation", 
    "speech-recognition", 
    "text-to-speech", 
    "llm", 
    "langchain", 
    "privacy-first"
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: End Users/Desktop",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Multimedia :: Sound/Audio :: Speech",
    "Topic :: Office/Business",
    "Topic :: Software Development :: Libraries :: Application Frameworks",
]
requires-python = ">=3.9,<3.12"

# Minimal core dependencies - only essential runtime requirements
dependencies = [
    # Configuration & Logging (always needed)
    "python-dotenv>=1.0.0",
    "coloredlogs>=15.0.0",
    "click>=8.1.0",
    "colorama>=0.4.6",
    
    # Basic utilities (always needed)
    "psutil>=5.0.0",
    "tenacity>=9.0.0",
    
    # Core data handling
    "pydantic>=2.10.0",
    "pydantic-settings>=2.7.0",
    "jsonschema>=4.24.0",
]

[project.optional-dependencies]
# ============================================================================
# APPROACH 1: Direct Script Execution - Full Runtime Dependencies
# ============================================================================
# For developers and advanced users who want full control
runtime = [
    # Audio Processing & Speech Recognition
    "faster-whisper==1.1.1",
    "RealtimeSTT==0.3.94",
    "PyAudio==0.2.14",
    "openwakeword==0.6.0",
    "webrtcvad-wheels==2.0.14",
    
    # Text-to-Speech
    "piper-tts==1.2.0",
    "piper-phonemize==1.1.0",
    "realtimetts==0.4.47",
    
    # Machine Learning & AI Core (NO TORCH - added separately)
    "numpy==1.26.4",
    "scipy==1.15.1",
    "onnxruntime==1.20.1",
    
    # LangChain & LLM Framework
    "langchain==0.3.25",
    "langchain-core==0.3.62",
    "langchain-community==0.3.27",
    "langchain-text-splitters==0.3.8",
    "langgraph==0.4.6",
    "langgraph-checkpoint==2.0.26",
    "langgraph-sdk==0.1.51",
    "langsmith==0.3.8",
    
    # MCP (Model Context Protocol) Integration
    "langchain-mcp-adapters>=0.1.8",
    
    # Database & Storage
    "aiosqlite>=0.19.0",
    "sqlite-vec",
    "SQLAlchemy==2.0.38",
    
    # Scheduling & Automation
    "croniter==6.0.0",
    
    # Web & Network
    "requests==2.32.4",
    "aiohttp==3.12.14",
    "httpx==0.28.1",
    "urllib3==2.5.0",
    "duckduckgo-search==7.5.2",
    
    # Utilities
    "halo==0.0.31",
    "tqdm==4.67.1",
    "emoji==2.14.0",
    "regex==2024.11.6",
    "Jinja2==3.1.6",
    "MarkupSafe==3.0.2",
    
    # ML Text Processing
    "ctranslate2==4.5.0",
    "tokenizers==0.21.0",
    "tiktoken==0.8.0",
    "sentence-transformers==3.0.0",
    "transformers",
    "huggingface-hub==0.30.2",
    "Pillow>=10.3.0",
]

# ============================================================================
# TORCH DEPENDENCIES - Choose based on hardware
# ============================================================================
# Default CPU torch (compatible with most systems)
torch-cpu = [
    "torch==2.8.0",
    "torchaudio==2.6.0",
    "torchvision==0.21.0",
]

# Note: runtime-no-torch section removed - 'runtime' no longer includes torch by default

# ============================================================================
# APPROACH 2: Executable Building - PyInstaller Support
# ============================================================================
# For building standalone executables for novice users
build = [
    "pyinstaller>=6.0.0",
    "auto-py-to-exe>=2.4.0",  # GUI for PyInstaller
    "pyinstaller-hooks-contrib>=2024.0",
    "toml>=0.10.2",  # For version parsing in build script
]

# Executable runtime (lighter than full runtime)
exe-runtime = [
    "aurora[runtime]",  # Include all runtime dependencies
]

# ============================================================================
# APPROACH 3: Container Support
# ============================================================================
container = [
    "gunicorn>=21.0.0",  # For production WSGI server
    "supervisor>=4.2.0",  # Process management in containers
]

# ============================================================================
# HARDWARE ACCELERATION OPTIONS
# ============================================================================
# CUDA acceleration support
# Note: CUDA packages installed via wheel installer with custom index URL
cuda = [
    "onnxruntime-gpu",
    # torch, torchaudio, torchvision CUDA versions installed separately by setup script
]

# ROCm (AMD) acceleration support  
# Note: ROCm packages installed via wheel installer with custom index URL
rocm = [
    "onnxruntime",
    # torch, torchaudio ROCm versions installed separately by setup script
]

# Metal (Apple Silicon) acceleration support
metal = [
    # Note: Uses CPU torch packages as Metal acceleration happens at llama-cpp-python level
    "torch==2.8.0",
    "torchaudio==2.6.0",
    "torchvision==0.21.0",
    "onnxruntime",
]

# Vulkan (cross-platform) acceleration support  
vulkan = [
    # Note: Same as CPU setup, acceleration happens at llama-cpp-python level
    "torch==2.8.0",
    "torchaudio==2.6.0", 
    "torchvision==0.21.0",
    "onnxruntime",
]

# SYCL (Intel GPU) acceleration support
sycl = [
    # Note: Same as CPU setup, acceleration happens at llama-cpp-python level
    "torch==2.8.0",
    "torchaudio==2.6.0",
    "torchvision==0.21.0", 
    "onnxruntime",
]

# RPC (remote/distributed) acceleration support
rpc = [
    # Note: Same as CPU setup, acceleration happens at llama-cpp-python level
    "torch==2.8.0",
    "torchaudio==2.6.0",
    "torchvision==0.21.0",
    "onnxruntime",
]

# ============================================================================
# LLM BACKENDS  
# ============================================================================
# Note: llama-cpp-python is handled separately by wheel installer
# These groups are deprecated - use installation scripts or wheel installer directly

# OpenAI integration
openai = [
    "openai==1.82.0",
    "langchain-openai==0.3.18",
]

# Local embeddings
embeddings-local = [
    "langchain-huggingface==0.2.0",
]

# ============================================================================
# OPTIONAL MODULES (from modules/ folder)
# ============================================================================
# UI Module (PyQt6 interface)
ui = [
    "PyQt6==6.6.1",
    "PyQt6-Qt6==6.6.1", 
    "PyQt6-sip==13.6.0",
    "markdown==3.8",
]

# Google Workspace integration (Gmail, Calendar)
google = [
    "langchain-google-community==2.0.7",
]

# Productivity tools
jira = [
    "atlassian-python-api",
]

github = [
    "pygithub",
]

slack = [
    "slack-sdk",
    "beautifulsoup4==4.13.3",
]

# Search engines
brave-search = [
    "brave-search",
]

# OpenRecall integration dependencies (from modules/openrecall)
openrecall = [
    "Flask==3.0.3",
    "mss==9.0.1",
    "shapely==2.0.4",
    "h5py==3.11.0", 
    "rapidfuzz==3.9.3",
    "python-doctr @ git+https://github.com/koenvaneijk/doctr.git@af711bc04eb8876a7189923fb51ec44481ee18cd",
]

# Platform-specific dependencies for OpenRecall
openrecall-windows = [
    "pywin32",
]

openrecall-macos = [
    "pyobjc==10.3",
]

# ============================================================================
# DEVELOPMENT DEPENDENCIES
# ============================================================================
dev = [
    "pytest==8.3.5",
    "pytest-asyncio",
    "pytest-cov",
    "black",
    "flake8",
    "mypy",
    "pre-commit",
    "ipython",
    "jupyter",
    "pytest-xdist",       # For parallel test execution
    "pytest-sugar",       # For better test progress visualization
    "pytest-clarity",     # For better test failure output
    "pytest-html",        # For HTML test reports
    "coverage[toml]",     # For comprehensive test coverage reporting
    
    # Release management
    "python-semantic-release>=10.4.1",  # Automated versioning and changelog
    "commitizen>=3.0.0",               # Enforce conventional commits
    "toml>=0.10.2",                    # For version file parsing
    "requests>=2.32.0",                # For GitHub API calls
]

# Core test dependencies needed for all test types
test = [
    "pytest==8.3.5",
    "pytest-asyncio",
    "pytest-cov",
    "pytest-mock",
    "httpx[testing]",
    "pytest-timeout",
    "pytest-benchmark",
]

# Dependencies for unit tests
test-unit = [
    "aurora[test]",
    "faker",
    "freezegun",
]

# Dependencies for integration tests (including database)
test-integration = [
    "aurora[test,test-unit]",
    "pytest-docker",
    "aiosqlite>=0.19.0",  # Used instead of pytest-sqlite which is not available
]

# Dependencies for end-to-end tests
test-e2e = [
    "aurora[test,test-unit]",
    "pytest-playwright",
    "selenium",
]

# Dependencies for performance tests
test-performance = [
    "aurora[test]",
    "pytest-benchmark",
    "locust",
    "psutil",
]

# All test dependencies combined
test-all = [
    "aurora[test,test-unit,test-integration,test-e2e,test-performance]",
]

# ============================================================================
# CONVENIENCE BUNDLES - GRANULAR CUDA/CPU OPTIONS
# ============================================================================

# ============================================================================
# SIMPLIFIED INSTALLATION OPTIONS
# ============================================================================

# ============================================================================
# 1. CORE INSTALLATION - Bare minimum dependencies only
# ============================================================================
# Absolute minimum - just what's required for Aurora to start
# Note: Still requires configuring an LLM provider (third-party or local)
core = [
    "aurora[runtime,torch-cpu]",  
]

# ============================================================================
# 2. THIRD-PARTY PROVIDERS - Use external API services (OpenAI, Anthropic)
# ============================================================================
# Third-party providers with API keys (recommended for beginners)
third-party = [
    "aurora[runtime,torch-cpu,openai]",  
]

# Third-party providers with additional features
third-party-full = [
    "aurora[runtime,torch-cpu,openai,embeddings-local,ui,google,jira,github,slack,brave-search,openrecall]",
]

# ============================================================================
# 3. LOCAL EXECUTION - Run models on your hardware
# ============================================================================
# Local models using HuggingFace transformers (automatic device handling)
local-huggingface = [
    "aurora[runtime,torch-cpu,embeddings-local]",  
]

# Local models using HuggingFace transformers with GPU acceleration
# Note: PyTorch GPU packages installed separately by setup script
local-huggingface-gpu = [
    "aurora[runtime,embeddings-local]",  # No torch - PyTorch installed separately by wheel installer
]

# Local models using optimized llama-cpp backend (CPU only)  
local-llama-cpu = [
    "aurora[runtime,embeddings-local]",
    # Note: llama-cpp-python[cpu] installed separately by setup script
]

# Local models using optimized llama-cpp backend with GPU acceleration
# Note: Specific backend (CUDA/ROCm/Metal) chosen during setup
local-llama-gpu = [
    "aurora[runtime,embeddings-local]",  # No torch conflicts, GPU backend chosen during setup
    # Note: llama-cpp-python with specific acceleration installed by setup script
]

# ============================================================================
# 4. COMPLETE INSTALLATIONS - All features included
# ============================================================================
# All features with third-party providers (easiest setup)
full-third-party = [
    "aurora[runtime,torch-cpu,openai,embeddings-local,ui,google,jira,github,slack,brave-search,openrecall]",
]

# All features with local HuggingFace models
full-local-huggingface = [
    "aurora[runtime,torch-cpu,embeddings-local,ui,google,jira,github,slack,brave-search,openrecall]",
]

# All features with local HuggingFace models + GPU acceleration
# Note: PyTorch GPU packages installed separately by setup script
full-local-huggingface-gpu = [
    "aurora[runtime,embeddings-local,ui,google,jira,github,slack,brave-search,openrecall]",  # No torch - PyTorch installed separately
]

# All features with optimized llama-cpp backend (CPU)
full-local-llama-cpu = [
    "aurora[runtime,embeddings-local,ui,google,jira,github,slack,brave-search,openrecall]",
    # Note: llama-cpp-python[cpu] installed separately by setup script
]

# All features with optimized llama-cpp backend + GPU acceleration
full-local-llama-gpu = [
    "aurora[runtime,embeddings-local,ui,google,jira,github,slack,brave-search,openrecall]",
    # Note: llama-cpp-python with specific GPU backend installed by setup script
]

# ============================================================================
# LEGACY COMPATIBILITY - Deprecated (use new options above)
# ============================================================================
# All features - auto-detect hardware (legacy compatibility)
all = [
    "aurora[full-llama-cuda]",  # Default to CUDA for backwards compatibility
]

# ============================================================================
# 5. DEVELOPMENT INSTALLATIONS - For contributors and advanced users
# ============================================================================
# Development with third-party providers (fastest setup for development)
dev-third-party = [
    "aurora[runtime,torch-cpu,openai,dev,test-all,build,container]",
]

# Development with local models (CPU - good for testing)
dev-local-cpu = [
    "aurora[runtime,torch-cpu,embeddings-local,dev,test-all,build,container]",
]

# Development with local models + GPU (for ML/AI development)
# Note: PyTorch GPU packages installed separately by setup script
dev-local-gpu = [
    "aurora[runtime,embeddings-local,dev,test-all,build,container]",  # No torch - PyTorch installed separately
]

[project.urls]
"Homepage" = "https://github.com/aurora-ai/aurora"
"Bug Reports" = "https://github.com/aurora-ai/aurora/issues"
"Source" = "https://github.com/aurora-ai/aurora"
"Documentation" = "https://github.com/aurora-ai/aurora/blob/main/readme.md"

[project.scripts]
aurora = "main:main"
aurora-build = "scripts.build:main"
aurora-setup = "scripts.setup:main"

[tool.setuptools]
packages = ["app", "modules"]

[tool.setuptools.package-data]
app = [
    "database/migrations/*.sql",
]
# Model files are now stored at root level and excluded from package builds
# voice_models/ and chat_models/ directories contain large model files
# Users should download/manage these separately

[tool.black]
line-length = 100
target-version = ['py311']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
  | __pycache__
)/
'''

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[[tool.mypy.overrides]]
module = [
    "faster_whisper.*",
    "RealtimeSTT.*",
    "realtimetts.*",
    "openwakeword.*",
    "pvporcupine.*",
    "pyaudio.*",
    "langchain.*",
    "langchain_core.*",
    "langchain_community.*",
    "langsmith.*",
    "langgraph.*",
    "sentence_transformers.*",
    "ctranslate2.*",
    "sklearn.*",
    "tensorrt.*",
    "mss.*",
    "PyQt6.*",
]
ignore_missing_imports = true

# ============================================================================
# PYTHON SEMANTIC RELEASE CONFIGURATION
# ============================================================================
[tool.semantic_release]
# Version management
version_toml = ["pyproject.toml:project.version"]
version_variables = [
    "app/__init__.py:__version__",
]
version_pattern = [
    "readme.md:Aurora Voice Assistant v{version}",
    "scripts/build.py:Aurora Voice Assistant v{version}",
]

# # Build command - create Python packages and executables
# build_command = """
# python -m pip install --upgrade pip
# pip install -e .[build,runtime,torch-cpu]
# python -m build --sdist --wheel .
# python scripts/build.py --target exe --clean
# """

# Upload to PyPI
upload_to_vcs_release = true
upload_to_pypi = false  # Disabled until package is ready for PyPI

# Commit parser for conventional commits
commit_parser = "conventional"

# Branch configuration for releases
[tool.semantic_release.branches.main]
match = "main"
prerelease = false

# Changelog configuration
[tool.semantic_release.changelog]

[tool.semantic_release.changelog.default_templates]
changelog_file = "CHANGELOG.md"
output_format = "md"

# Exclude commits that don't need changelog entries
exclude_commit_patterns = [
    '''chore(?:\([^)]*?\))?: .+''',
    '''ci(?:\([^)]*?\))?: .+''', 
    '''refactor(?:\([^)]*?\))?: .+''',
    '''style(?:\([^)]*?\))?: .+''',
    '''test(?:\([^)]*?\))?: .+''',
    '''build\((?!deps\): .+)''',
    '''Merge .*''',
    '''Initial [Cc]ommit.*''',
    '''Bump to version.*''',
    '''Version bump.*''',
]

# Custom sections for changelog
[tool.semantic_release.changelog.sections]
feature = { title = "🚀 Features", description = "New features and enhancements" }
fix = { title = "🐛 Bug Fixes", description = "Bug fixes and corrections" }
breaking = { title = "💥 Breaking Changes", description = "Breaking changes that require attention" }
documentation = { title = "📚 Documentation", description = "Documentation improvements" }
performance = { title = "⚡ Performance", description = "Performance improvements" }
refactor = { title = "🔧 Code Refactoring", description = "Code improvements without feature changes" }
build = { title = "🏗️ Build System", description = "Build and dependency updates" }

# GitHub releases
[tool.semantic_release.remote]
name = "origin"
type = "github"
ignore_token_for_push = false
token = { env = "GITHUB_TOKEN" }
url = "https://github.com/joaojhgs/aurora"

# Release notes customization  
[tool.semantic_release.changelog.environment]
block_start_string = "{%"
block_end_string = "%}"
variable_start_string = "{{"
variable_end_string = "}}"
comment_start_string = "{#"
comment_end_string = "#}"
trim_blocks = false
lstrip_blocks = false

# ============================================================================
# COMMITIZEN CONFIGURATION  
# ============================================================================
[tool.commitizen]
name = "cz_conventional_commits"
tag_format = "v$version"
version_scheme = "semver"
version_provider = "pep621"
update_changelog_on_bump = true
major_version_zero = false

# Custom commit types for Aurora project
[tool.commitizen.customize]
message_template = """{{change_type}}{{scope}}: {{message}}

{{body}}

{{footer}}"""

example = "feat(speech): add wake word detection"

schema = """
<type>(<scope>): <subject>

<body>

<footer>
"""

schema_pattern = '''(?s)(build|ci|docs|feat|fix|perf|refactor|style|test|chore|revert)(\(.+?\))?: (.+?)(\n\n.*)?'''

bump_pattern = "^(break|new|fix|hotfix)"

[tool.commitizen.customize.bump_map]
"break" = "MAJOR"
"new" = "MINOR"
"fix" = "PATCH" 
"hotfix" = "PATCH"

info_path = "cz_info.txt"
info = """
Aurora Voice Assistant follows conventional commits for automated releases:

Types:
- feat: New features
- fix: Bug fixes  
- docs: Documentation changes
- style: Code formatting changes
- refactor: Code restructuring without feature changes
- perf: Performance improvements
- test: Test changes
- build: Build system or dependency changes
- ci: CI configuration changes
- chore: Other changes that don't modify src or test files
- revert: Reverts a previous commit

Scopes (optional):
- speech: Speech recognition and processing
- tts: Text-to-speech functionality  
- llm: Language model integration
- ui: User interface
- config: Configuration management
- database: Database operations
- scheduler: Task scheduling
- tools: Tool integrations (Jira, Slack, etc.)

Examples:
- feat(speech): add new wake word detection
- fix(llm): resolve memory leak in chat processing
- docs: update installation guide
- style(ui): format Qt interface code
"""
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config", 
    "--verbose",
    "-m", "not performance"  # Don't run performance tests by default
]

[tool.autopep8]
max_line_length = 100
aggressive = 3
ignore = "E501,W503,E203"
in-place = true
recursive = true

markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "e2e: marks tests as end-to-end tests",
    "performance: marks tests as performance tests",
    "db: marks tests that require database access",
    "network: marks tests that require network access",
    "external_api: marks tests that call external APIs",
    "simple: marks simplified versions of tests that use mocks instead of real dependencies",
    "smoke: marks critical smoke tests that must pass for basic functionality",
    "flaky: marks tests that might be unreliable",
    "mocked: marks tests that use mock objects instead of real dependencies",
    "langgraph: tests related to the langgraph module",
    "scheduler: tests related to the scheduler module",
    "stt: tests related to the speech-to-text module",
    "tts: tests related to the text-to-speech module", 
]
# Default timeout for tests in seconds (prevents hanging tests)
timeout = 300
