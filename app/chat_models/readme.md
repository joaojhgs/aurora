## You can download and place here custom models to run locally
## Currently we only support GGUF models for llama-cpp, for quantized and eficient run